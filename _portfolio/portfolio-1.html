---
title: Tesla Îî∞ÎùºÌïòÍ∏∞
excerpt: "Integrating object detection, lane detection, depth estimation, and driving algorithms to develop an autonomous vehicle.<br/><img src='/images/selfdrive.png'>"
collection: portfolio
---

<br>

<div style="text-align: center;">
    <iframe src="https://chehun16.github.io/deepdrive/" width="800" height="400" 
            style="display: block; margin: auto; border: none;"></iframe>
</div>


<p style="color: gray; font-size: 0.8em;">
    Detailed information about the project can be found in the 
    <a href="https://chehun16.github.io/deepdrive/" target="_blank" style="color: gray; text-decoration: underline;">
        project page
    </a> above!
</p>


<br>

<h2>Project Overview</h2>

<p align="center">
    <img src="/images/selfdrive.png" alt="Self-driving Project" width="600">
</p>

<p>
I worked on a project that integrated object detection, lane detection, depth estimation, and planning algorithms 
to develop an autonomous driving system. Through the perception stage, we processed the obtained information to generate an 
occupancy map and planned the control process accordingly.
</p>

<p align="center">
  <img src="/images/all.png" alt="Self-driving Project" width="600">
</p>

<p>
For object detection, we used <strong>YOLOv8</strong>, while <strong>UFLD</strong> was used for lane detection, 
and <strong>Metric3D</strong> for depth estimation. In the planning stage, we implemented path generation using the 
<strong>A* algorithm</strong>.
</p>

<p>
To improve inference speed, we converted the <strong>PyTorch</strong> models to <strong>TensorRT</strong>. 
After developing the object, lane, depth, and planning engines, we integrated them into a single unified engine.
We named the unified engine as the <strong>Autonomous Engine</strong>.
</p>

<p>
For our project, we used the Nvidia Jetson Orin Nano 8GB, Nvidia JetRacer, and a CSI camera. 
While driving the JetRacer, we collected a total of 11,243 images. After capturing the data, we manually 
labeled objects and lanes before conducting experiments.
</p>

<h2>Results</h2>

<p align="center">
    <video width="600" controls>
        <source src="/videos/realfinal.mp4" type="video/mp4">
        Your browser does not support the video tag.
    </video>
</p>

<p>This is a visualization of the results when running the Autonomous Engine.</p>

<p>During this project, the vehicle had difficulty accurately detecting lanes, so we used <strong>ResNet</strong> to train 
    the direction it should follow. This video visualizes the results.</p>

<p align="center">
    <video width="600" controls>
        <source src="/videos/resnet3.mp4" type="video/mp4">
        Your browser does not support the video tag.
    </video>
</p>

<br>

<h2>Seminar</h2>

<p>I was in charge of the presentation at the 10th <strong style="color: #264a8e">deepdaiv</strong> Open Seminar.üòä</p>

<p align="center">
    <img src="/images/presentation.png" alt="Self-driving Project" width="600">
</p>

<br>
<br>
<br>

<div style="border-left: 6px solid #a8aeb5; background-color: #f0f8ff; padding: 15px; margin-top: 20px; border-radius: 5px; font-size: 14px;">
    <strong>üßë‚Äçüíª My Role:</strong> Led the development of object detection and lane detection modules, managed data collection and annotation, and implemented the integration of all perception and planning components into a unified Autonomous Engine. Additionally, I delivered a presentation of the project at the deepdaiv Open Seminar.
</div>

<br>

<a href="https://github.com/chehun16/autonomous-engine" style="text-decoration: none; display: inline-flex; align-items: center; padding: 6px 10px; background-color: #333; color: white; border-radius: 5px; font-size: 14px; font-weight: bold;">
    <img src="https://github.com/fluidicon.png" alt="GitHub" style="width: 18px; height: 18px; margin-right: 5px; filter: invert(1);">
    autonomous-engine
</a>